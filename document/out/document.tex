\documentclass{article}
% generated by Madoko, version 1.0.0-rc5
%mdk-data-line={1}


\usepackage[heading-base={2},section-num={False},bib-label={True}]{madoko2}


\begin{document}



%mdk-data-line={7}
\mdxtitleblockstart{}
%mdk-data-line={7}
\mdxtitle{\mdline{7}Optimization of a DNN program on the CPU+MIC}%mdk
\mdxauthorstart{}
%mdk-data-line={12}
\mdxauthorname{\mdline{12}University of Electronic Secience and Technology of China}%mdk
\mdxauthorend\mdtitleauthorrunning{}{}\mdxtitleblockend%mdk

%mdk-data-line={9}
\begin{abstract}%mdk

%mdk-data-line={10}
\noindent\mdline{10}This article is a part of competition proposal of Asia Supercomputer Student Challenge. We analysis the \mdline{10}\mdcode{DNN}\mdline{10} program, put forward different optimization methods, test them and point their pros and cons. In the end we talk about our limitations.%mdk
%mdk
\end{abstract}%mdk

%mdk-data-line={12}
\section{\mdline{12}1.\hspace*{0.5em}\mdline{12}Introduction}\label{sec-introduction}%mdk%mdk

%mdk-data-line={13}
\noindent\mdline{13}There is a program based on a standalone hybrid CPU+MIC platform called \mdline{13}\mdcode{DNN(deep~neural~network)}\mdline{13} needed to be parallelized for obtain better performance. Here is some detailed information about hardware in Figure 1, software configuration in Figure 2.%mdk

%mdk-data-line={15}
\mdline{15}After optimization, the final program is tested on one computing server in the CPU+MIC hybrid cluster. Performance analysis in this proposal is based on the results of this test.%mdk

%mdk-data-line={16}
\begin{figure}[tbp]%mdk
\begin{mdcenter}%mdk

%mdk-data-line={18}
\noindent\mdline{18}\includegraphics[keepaspectratio=true,width=\dimmin{}{\dimwidth{0.90}}]{images/2016-02-18-23-01-13-}{}\mdline{18}%mdk

%mdk-data-line={21}
\mdhr{}%mdk

%mdk-data-line={22}
\noindent\mdline{22}\mdcaption{\textbf{Figure~\mdcaptionlabel{1}.}~\mdcaptiontext{Hardware configuration}}%mdk
%mdk
\end{mdcenter}\label{fig-myfigure}%mdk
%mdk
\end{figure}%mdk

%mdk-data-line={23}
\begin{figure}[tbp]%mdk
\begin{mdcenter}%mdk

%mdk-data-line={24}
\noindent\mdline{24}\includegraphics[keepaspectratio=true,width=\dimmin{}{\dimwidth{0.90}}]{images/2016-02-18-23-13-11-}{}\mdline{24}%mdk

%mdk-data-line={27}
\mdhr{}%mdk

%mdk-data-line={28}
\noindent\mdline{28}\mdcaption{\textbf{Figure~\mdcaptionlabel{2}.}~\mdcaptiontext{Software configuration}}%mdk
%mdk
\end{mdcenter}\label{fig-myfigure}%mdk
%mdk
\end{figure}%mdk

%mdk-data-line={29}
\section{\mdline{29}2.\hspace*{0.5em}\mdline{29}Analysis of the serial program}\label{sec-analysis-of-the-serial-program}%mdk%mdk

%mdk-data-line={30}
\subsection{\mdline{30}2.1.\hspace*{0.5em}\mdline{30}Coarse grain analysis}\label{sec-coarse-grain-analysis}%mdk%mdk

%mdk-data-line={31}
\noindent\mdline{31}At first, we generate a call graph(Figure 3) by using \mdline{31}\mdcode{Google~perfools}\mdline{31}, a open source performance profiler, to have a glance though it. Every square represents a function, and the bigger square is, the more time corresponding function cost.%mdk

%mdk-data-line={33}
\begin{figure}[tbp]%mdk
\begin{mdcenter}%mdk

%mdk-data-line={34}
\noindent\mdline{34}\includegraphics[keepaspectratio=true,width=\dimmin{}{\dimwidth{0.90}}]{images/100001994364201}{}\mdline{34}%mdk

%mdk-data-line={37}
\mdhr{}%mdk

%mdk-data-line={38}
\noindent\mdline{38}\mdcaption{\textbf{Figure~\mdcaptionlabel{3}.}~\mdcaptiontext{Google Perfools results}}%mdk
%mdk
\end{mdcenter}\label{fig-myfigure}%mdk
%mdk
\end{figure}%mdk

%mdk-data-line={38}
\mdline{38}Obviously, the hot spot is something about \mdline{38}\mdcode{MKL}\mdline{38}. After googling
and searching Intel document we know that MKL provides \mdline{39}\mdcode{BLAS~routinues}\mdline{39}, which includes a serial function named \mdline{39}\mdcode{cblas\_?gemm}\mdline{39}
to compute a matrix-matrix product with general matrices.%mdk

%mdk-data-line={42}
\mdline{42}But giving that MKL function is well-optimized, we search for all position where \mdline{42}\mdcode{cblas\_*gemm}\mdline{42} is called. Results show the usage of \mdline{42}\mdcode{cblas\_*gemm}\mdline{42} appear in file \mdline{42}\mdcode{dnn\_func.cpp}\mdline{42}, more specifically, in three functions:%mdk

%mdk-data-line={44}
\begin{itemize}[noitemsep,topsep=\mdcompacttopsep]%mdk

%mdk-data-line={44}
\item\mdline{44}\mdcode{\preindent{1}extern~"C"~int~dnnForward(NodeArg~\&nodeArg)}\mdline{44}%mdk

%mdk-data-line={45}
\item\mdline{45}\mdcode{\preindent{1}extern~"C"~int~dnnBackward(NodeArg~\&nodeArg)}\mdline{45}%mdk

%mdk-data-line={46}
\item\mdline{46}\mdcode{\preindent{1}extern~"C"~int~dnnUpdate(NodeArg~\&nodeArg)}\mdline{46}

%mdk-data-line={47}
\begin{figure}[tbp]%mdk
\begin{mdcenter}%mdk

%mdk-data-line={48}
\noindent\mdline{48}\includegraphics[keepaspectratio=true,width=\dimmin{}{\dimwidth{0.90}}]{images/2016-02-19-10-36-25-}{}\mdline{48}%mdk

%mdk-data-line={50}
\mdhr{}%mdk

%mdk-data-line={51}
\noindent\mdline{51}\mdcaption{\textbf{Figure~\mdcaptionlabel{4}.}~\mdcaptiontext{Intel VTune top-down tree}}%mdk
%mdk
\end{mdcenter}\label{fig-myfigure}%mdk
%mdk
\end{figure}%mdk%mdk
%mdk
\end{itemize}%mdk

%mdk-data-line={52}
\noindent\mdline{52}They call \mdline{52}\mdcode{MKL}\mdline{52} function \mdline{52}\mdcode{cblas\_sgemm}\mdline{52} many times by \mdline{52}\mdcode{for~loop}\mdline{52} and 
cost almost 90\% of all CPU time. So we guess that those function
 is what we may optimize, aka, hotspots.
The report(see Figure 4.) showed by\mdline{55}\mdcode{Intel~VTune}\mdline{55}, another profiler, proves our guess.%mdk

%mdk-data-line={59}
\mdline{59}According to a skim through the source code, we could establish a clear structure about this program. To simplify code, original program could be rewritten in pseudocode:%mdk
\begin{mdpre}%mdk
\noindent{\mdcolor{purple}1}.~{\mdcolor{teal}GetInitFileConfig}(cpuArg)\\
{\mdcolor{purple}2}.~{\mdcolor{teal}While}~{\mdcolor{teal}FetchOneChunk}(cpuArg,~onChunk)~{\mdcolor{navy}do}:\\
{\mdcolor{purple}3}.~~~~~~~{\mdcolor{teal}While}~{\mdcolor{teal}FetchOneBunch}(oneChunk,~nodeArg)~{\mdcolor{navy}do}:\\
{\mdcolor{purple}4}.~~~~~~~~~~~~dnnForward(nodeArg)\\
{\mdcolor{purple}5}.~~~~~~~~~~~~dnnBackward(nodeArg)\\
{\mdcolor{purple}6}.~~~~~~~~~~~~dnnUpate(nodeArg)\\
{\mdcolor{purple}7}.~{\mdcolor{teal}WriteWts}(nodeArg,~cpuArg)\\
{\mdcolor{purple}8}.~{\mdcolor{teal}UninitProgramConfig}(cpuArg)\\
%mdk
\end{mdpre}\noindent\mdline{71}There are two nested loop before \mdline{71}\mdcode{dnn*()}\mdline{71} series, and 
in each of those processing function many matrix-matrix product are
executed. Whether those hotspots could be parallelized or not depends
on data scale, dependency and so on. Before we discuss some methods and weighed their pros and cons the implementation of \mdline{74}\mdcode{DNN}\mdline{74} should be most carefully checked.

%mdk-data-line={76}
\subsection{\mdline{76}2.2.\hspace*{0.5em}\mdline{76}Fine grain analysis}\label{sec-fine-grain-analysis}%mdk%mdk

%mdk-data-line={78}
\subsubsection{\mdline{78}2.2.1.\hspace*{0.5em}\mdline{78}Matrix size}\label{sec-matrix-size}%mdk%mdk

%mdk-data-line={80}
\noindent\mdline{80}All \mdline{80}\mdcode{cblas\_sgemm}\mdline{80} is called like this:%mdk
\begin{mdpre}%mdk
\noindent~~cblas\_sgemm({\mdcolor{teal}CblasRowMajor},~{\mdcolor{teal}CblasNoTrans},~{\mdcolor{teal}CblasNoTrans},\textbackslash{}\\
~~~~~~~~numN,~numA[i],~numA[i-{\mdcolor{purple}1}],~\textbackslash{}\\
~~~~~~~~one,~d\_Y[i-{\mdcolor{purple}1}],~numA[i-{\mdcolor{purple}1}],~d\_W[i],~numA[i],~one,~d\_Y[i],~numA[i]);%mdk
\end{mdpre}\noindent\mdline{86}The arguments \mdline{86}\mdcode{numN}\mdline{86}, \mdline{86}\mdcode{numA[i]}\mdline{86}, \mdline{86}\mdcode{numA[i-1]}\mdline{86} indicating the size of the matrices:

%mdk-data-line={88}
\begin{itemize}[noitemsep,topsep=\mdcompacttopsep]%mdk

%mdk-data-line={88}
\item\mdline{88}\mdcode{d\_Y[i-1]}\mdline{88} is a \mdline{88}\mdcode{numN}\mdline{88} row by \mdline{88}\mdcode{numA[i]}\mdline{88} column matrix;%mdk

%mdk-data-line={89}
\item\mdline{89}\mdcode{d\_W[i]}\mdline{89} is a \mdline{89}\mdcode{numN}\mdline{89} row by \mdline{89}\mdcode{numA[i-1]}\mdline{89} column matrix;%mdk

%mdk-data-line={90}
\item\mdline{90}\mdcode{d\_Y[i]}\mdline{90} is a \mdline{90}\mdcode{numA[i-1]}\mdline{90} row by \mdline{90}\mdcode{numA[i]}\mdline{90} column matrix.%mdk
%mdk
\end{itemize}%mdk

%mdk-data-line={92}
\noindent\mdline{92}As we known the bigger matrix size is, the higher degree of \mdline{92}\mdcode{MKL}\mdline{92} parallelism is. But in the \mdline{92}\mdcode{DNN}\mdline{92} program, the size of matrix is decided by \mdline{92}\mdcode{bunchSize}\mdline{92}, a constant integer (\mdline{92}\ensuremath{\approx}\mdline{92}1024), and element (\mdline{92}\ensuremath{\approx}\mdline{92}1024) of \mdline{92}\mdcode{dnnLayerArr}\mdline{92}, a constant integer array. The two integers are configured by specified file, and we are not allowed to modify it. For this reason there are no sufficiently large matrix to enable \mdline{92}\mdcode{auto~offload~model}\mdline{92} to speed up \mdline{92}\mdcode{DNN}\mdline{92}.\mdline{92}[\mdcite{mkl-mic}{2}]\mdline{92}%mdk

%mdk-data-line={94}
\subsubsection{\mdline{94}2.2.2.\hspace*{0.5em}\mdline{94}Cycles index}\label{sec-cycles-index}%mdk%mdk

%mdk-data-line={95}
\noindent\mdline{95}In the \mdline{95}\mdcode{dnn*}\mdline{95} series every loop call \mdline{95}\mdcode{cblas\_sgemm}\mdline{95} \mdline{95}\mdcode{numN}\mdline{95}(\mdline{95}\ensuremath{\approx}\mdline{95}7) times, which indicates the length of \mdline{95}\mdcode{dnnLayerArr}\mdline{95}. It\mdline{95}'\mdline{95}s regretful that the value cannot be modified by us. Giving the number of core(\mdline{95}\ensuremath{\approx}\mdline{95}24 in CPU or \mdline{95}\ensuremath{\approx}\mdline{95}60 in MIC) and constant \mdline{95}\mdcode{numN}\mdline{95}, it\mdline{95}'\mdline{95}s not wise to parallelize those loops.%mdk

%mdk-data-line={97}
\section{\mdline{97}3.\hspace*{0.5em}\mdline{97}Parallelization design methods}\label{sec-parallelization-design-methods}%mdk%mdk

%mdk-data-line={99}
\subsection{\mdline{99}3.1.\hspace*{0.5em}\mdline{99}Fine grain parallelism}\label{sec-fine-grain-parallelism}%mdk%mdk

%mdk-data-line={101}
\noindent\mdline{101}In function \mdline{101}\mdcode{dnnForward}\mdline{101}, it\mdline{101}'\mdline{101}s easy to observe there is a \mdline{101}\mdcode{for~loop}\mdline{101} calling \mdline{101}\mdcode{cblas\_sgemm}\mdline{101}, which nearly cost all CPU time consumed by this function. So it\mdline{101}'\mdline{101}s same with the \mdline{101}\mdcode{dnnBackward}\mdline{101} and \mdline{101}\mdcode{dnnUpdate}\mdline{101}. A rough thoughts occurred to us that we could parallelize this \mdline{101}\mdcode{for~loop}\mdline{101} using multi-threads. But data dependency in \mdline{101}\mdcode{for~loop}\mdline{101} and relatively small cycle index make it inefficient to parallelize. So   we should consider generalizing the parallelization region to smaller scale. Taking the feature of \mdline{101}\mdcode{MIC}\mdline{101} in to account we hope to execute highly parallel code and/or compute intensive code in the \mdline{101}\mdcode{MIC}\mdline{101}. Focusing on \mdline{101}\mdcode{dnn*}\mdline{101} series it\mdline{101}'\mdline{101}s easy to find an abstract structure to generalize them:%mdk
\begin{mdpre}%mdk
\noindent~~extern~{\mdcolor{maroon}"}{\mdcolor{maroon}C}{\mdcolor{maroon}"}~{\mdcolor{navy}int}~dnn***({\mdcolor{teal}NodeArg}~\&nodeArg)\\
\{\\
~~~~{\mdcolor{darkgreen}/*}{\mdcolor{darkgreen}~Variables~definition~}{\mdcolor{darkgreen}*/}\\
~~~~{\mdcolor{navy}float}~~*d\_X~=~nodeArg.d\_X;\\
~~~~~~...\\
~~~~{\mdcolor{darkgreen}/*}{\mdcolor{darkgreen}~Preprcess~function~func1~}{\mdcolor{darkgreen}*/}\\
~~~~func1(...);\\
\\
~~~{\mdcolor{darkgreen}/*}{\mdcolor{darkgreen}~a~for~loop~where~a~function,~a~MKL~and~another~function~are~invoked~oderly~}{\mdcolor{darkgreen}*/}\\
~~~{\mdcolor{navy}for}~({\mdcolor{navy}int}~i~=~num;~...)~\{\\
~~~~~~~~func2(...);\\
~~~~~~~~cblas\_sgemm({\mdcolor{teal}CblasRowMajor},~{\mdcolor{teal}CblasNoTrans},~...);\\
~~~~~~~~func3(...);\\
~~~~~~~~\}\\
~~~~\}~\\
~~~~{\mdcolor{navy}return}~{\mdcolor{purple}0};\\
\}\\
%mdk
\end{mdpre}\noindent\mdline{122}Arguments of \mdline{122}\mdcode{func1}\mdline{122}, \mdline{122}\mdcode{func2}\mdline{122}, \mdline{122}\mdcode{func3}\mdline{122} is matrix or vector, which is easy to parallelized. Taking all factors into consider, we have two choose: a) use serial \mdline{122}\mdcode{MKL}\mdline{122} then parallelize the whole \mdline{122}\mdcode{for~loop}\mdline{122}, b) use multi-thread \mdline{122}\mdcode{MKL}\mdline{122} and parallelize \mdline{122}\mdcode{func1,~2,~3}\mdline{122}. The two measures all support \mdline{122}\mdcode{MIC}\mdline{122}, but we prefer the b) because we could benefit from the parallel optimization of \mdline{122}\mdcode{MKL}\mdline{122} and cooperation among \mdline{122}\mdcode{MKL}\mdline{122} and \mdline{122}\mdcode{MIC}\mdline{122}.

%mdk-data-line={124}
\begin{figure}[tbp]%mdk
\begin{mdcenter}%mdk

%mdk-data-line={125}
\noindent\mdline{125}\includegraphics[keepaspectratio=true,width=\dimmin{}{\dimwidth{0.90}}]{images/2016-02-21-21-15-24-}{}\mdline{125}%mdk

%mdk-data-line={128}
\mdhr{}%mdk

%mdk-data-line={129}
\noindent\mdline{129}\mdcaption{\textbf{Figure~\mdcaptionlabel{5}.}~\mdcaptiontext{\mdcode{DNN} structure}}%mdk
%mdk
\end{mdcenter}\label{fig-figure}%mdk
%mdk
\end{figure}%mdk

%mdk-data-line={131}
\subsection{\mdline{131}3.2.\hspace*{0.5em}\mdline{131}Coarse grain parallelism}\label{sec-coarse-grain-parallelism}%mdk%mdk

%mdk-data-line={132}
\noindent\mdline{132}To implenment coarse grain parallelism we hope that each thread/process finish large subcomponents. To achieve this goal \mdline{132}\mdcode{DNN}\mdline{132} program should be divided into (mostly) independent and similar proportions, and every proportion should be as large as possible. But considering the structure of \mdline{132}\mdcode{DNN}\mdline{132} the default ordering of \mdline{132}\mdcode{dnn*}\mdline{132} series couldn\mdline{132}'\mdline{132}t be changed, neither does the processing of file-reading. So in our opinion it\mdline{132}'\mdline{132}s difficult to implement coarse grain parallelism without any change to \mdline{132}\mdcode{DNN}\mdline{132} structure and its dataset.%mdk

%mdk-data-line={134}
\section{\mdline{134}4.\hspace*{0.5em}\mdline{134}Performance optimization methods}\label{sec-performance-optimization-methods}%mdk%mdk

%mdk-data-line={136}
\subsection{\mdline{136}4.1.\hspace*{0.5em}\mdline{136}Serial \mdline{136}\mdcode{MKL}\mdline{136} function with \mdline{136}\mdcode{OpenMP}}\label{sec-serial-mkl-function-with-openmp}%mdk%mdk

%mdk-data-line={137}
\noindent\mdline{137}\mdcode{MKL}\mdline{137} function could decide whether to be threaded after a runtime check\mdline{137}[\mdcite{paramkl}{3}]\mdline{137}. To maximize the utilization it\mdline{137}'\mdline{137}s better to execute multithreading application in \mdline{137}\mdcode{MIC}\mdline{137}, but as we mentioned above there isn\mdline{137}'\mdline{137}t large enough cycles index to improve performance.%mdk

%mdk-data-line={139}
\subsection{\mdline{139}4.2.\hspace*{0.5em}\mdline{139}Compiler assisted offload}\label{sec-compiler-assisted-offload}%mdk%mdk

%mdk-data-line={140}
\noindent\mdline{140}To make the \mdline{140}\mdcode{DNN}\mdline{140} program scalable we use \mdline{140}\mdcode{offload~if}\mdline{140} the decide whether to run in the \mdline{140}\mdcode{MIC}\mdline{140} or not. After testing a 2048 \mdline{140}*\mdline{140} 2048 matrix or bigger is suitable to transmit to \mdline{140}\mdcode{MIC}\mdline{140} for better performance.%mdk

%mdk-data-line={142}
\subsection{\mdline{142}4.3.\hspace*{0.5em}\mdline{142}Environmental variables settings}\label{sec-environmental-variables-settings}%mdk%mdk

%mdk-data-line={143}
\begin{itemize}%mdk

%mdk-data-line={143}
\item{}
%mdk-data-line={143}
\mdline{143}\mdcode{MKL\_DYNAMIC=true}\mdline{143}:\mdline{143}\mdbr
\mdline{144}This option may reduce possible oversubscription from MKL threading. This option leads to a dynamic reduction of number of OpenMP\mdline{144}*\mdline{144} threads based on analysis of system workload.%mdk%mdk

%mdk-data-line={146}
\item{}
%mdk-data-line={146}
\mdline{146}\mdcode{MKL\_NUM\_THREADS=224}\mdline{146}:\mdline{146}\mdbr
\mdline{147}\mdcode{MKL}\mdline{147} is designed for high degree parallelization code, so we set the variables to enable \mdline{147}\mdcode{MKL}\mdline{147} threads.\mdline{147}[\mdcite{mkl_threads}{1}]\mdline{147} It\mdline{147}'\mdline{147}s necessary because we choose sequential \mdline{147}\mdcode{for~loop}\mdline{147} to call \mdline{147}\mdcode{MKL}\mdline{147} function.%mdk%mdk

%mdk-data-line={149}
\item{}
%mdk-data-line={149}
\mdline{149}\mdcode{MIC\_USE\_2MB\_BUFFERS=100M}\mdline{149}%mdk%mdk

%mdk-data-line={150}
\item{}
%mdk-data-line={150}
\mdline{150}\mdcode{MKL\_MIC\_ENABLE=FALSE}\mdline{150}:\mdline{150}\mdbr
\mdline{151}2MB pages are also needed. the 2MB pages in compiler-assisted offload are used to improve data transfer performance\mdline{151}[\mdcite{page_file}{4}]\mdline{151}, and they could be enabled using the \mdline{151}\mdcode{MIC\_USE\_2MB\_BUFFERS}\mdline{151} variable.%mdk%mdk
%mdk
\end{itemize}%mdk

%mdk-data-line={154}
\section{\mdline{154}5.\hspace*{0.5em}\mdline{154}Testing process and results on the CPU+MIC platform}\label{sec-testing-process-and-results-on-the-cpumic-platform}%mdk%mdk

%mdk-data-line={155}
\section{\mdline{155}6.\hspace*{0.5em}\mdline{155}Limitation}\label{sec-limitation}%mdk%mdk

%mdk-data-line={157;out/document-bib.bbl.mdk:1}
%mdk-data-line={157;out/document-bib.bbl.mdk:2}
\mdsetrefname{References}%mdk
{\mdbibindent{0}%mdk
\begin{thebibliography}{4}%mdk
\label{sec-bibliography}%mdk

%mdk-data-line={reference.bib:23}
\bibitem{mkl_threads}Konstantin Arturov. \emph{Recommended Settings for Calling Intel MKL Routines from Multi-Threaded Applications}. Intel, https://software.intel.com/en-us/articles/recommended-settings-for-calling-intel-mkl-routines-from-multi-threaded-applications.\label{mkl_threads}%mdk%mdk

%mdk-data-line={reference.bib:1}
\bibitem{mkl-mic}Noah Clemons. \emph{Recommendations to Choose the Right MKL Usage Model for Xeon Phi}. Intel, https://software.intel.com/en-us/articles/recommendations-to-choose-the-right-mkl-usage-model-for-xeon-phi. Mar. 2013.\label{mkl-mic}%mdk%mdk

%mdk-data-line={reference.bib:12}
\bibitem{paramkl}Intel. \emph{Parallelism in the Intel® Math Kernel Library}. https://software.intel.com/en-us/articles/parallelism-in-the-intel-math-kernel-library.\label{paramkl}%mdk%mdk

%mdk-data-line={reference.bib:31}
\bibitem{page_file}Zhang Z.~\emph{Performance Tips of Using Intel® MKL on Intel® Xeon Phi™ Coprocessor}. Intel, https://software.intel.com/en-us/articles/performance-tips-of-using-intel-mkl-on-intel-xeon-phi-coprocessor.\label{page_file}%mdk%mdk
\par%mdk
\end{thebibliography}}%mdk%mdk%mdk


\end{document}
