Title         : Optimization of the DNN program on the CPU+MIC Platform
Author        : University of Electronic Secience and Technology of China
Logo          : False

[TITLE]
# Preface
In the section a program based on a standalone hybrid CPU+MIC platform 
called `DNN(deep neural network)` should be parallelized to obtain
better performance. The detailed configuration about is as follows: 

![2016-02-18 23:01:13屏幕截图]

[2016-02-18 23:01:13屏幕截图]: images/2016-02-18-23-01-13-.png "2016-02-18 23:01:13屏幕截图" { width:auto; max-width:90% }

![2016-02-18 23:13:11屏幕截图]

[2016-02-18 23:13:11屏幕截图]: images/2016-02-18-23-13-11-.png "2016-02-18 23:13:11屏幕截图" { width:auto; max-width:90% }



# Analysis of the serial program
First, a call graph is generated by using `Google perfools`,
 a open source performance profiler, to have a glance though it. Every
 square represents a function, and the bigger square is, the more time
 corresponding function cost.
 

![100001994364201]

[100001994364201]: images/100001994364201.jpg "100001994364201" { width:auto; max-width:90% }

Obviously, the hot spot is something about `MKL`. After googling
and searching Intel document we know that MKL provides `BLAS routinues`,
which includes a serial funcition named `cblas_?gemm`
to compute a matrix-matrix product with general matrices.

Then we search for `cblas_*gemm`, results show the usage of `cblas_*gemm`
 appear in file `dnn_func.cpp`, more specifically, in three function:

* ` extern "C" int dnnForward(NodeArg &nodeArg)`
* ` extern "C" int dnnBackward(NodeArg &nodeArg)`
* ` extern "C" int dnnUpdate(NodeArg &nodeArg)`

They all call `MKL` function `cblas_sgemm` many times and 
cost almost 90% of all runtime. So we guess that those function
 is what we should optimize, aka, hotspots.
The report showed by`Intel VTune`, another profiler, proves our guess.

![2016-02-19 10:36:25屏幕截图]

[2016-02-19 10:36:25屏幕截图]: images/2016-02-19-10-36-25-.png "2016-02-19 10:36:25屏幕截图" { width:auto; max-width:90% }

After a skim through the source code, a clear structure 
about the program is establish in our heart. 
To simplify our describe, original program could be rewritten 
in pseudocode:

``` Java
1. GetInitFileConfig(cpuArg)
2. While FetchOneChunk(cpuArg, onChunk) do:
         While FetchOneBunch(oneChunk, nodeArg) do:
              dnnForward(nodeArg)
              dnnBackward(nodeArg)
              dnnUpate(nodeArg)
3. WriteWts(nodeArg, cpuArg)
4. UninitProgramConfig(cpuArg)

```

Then 