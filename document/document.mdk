Title         : Optimization of the DNN program on the CPU+MIC Platform
Author        : University of Electronic Secience and Technology of China
Logo          : False

[TITLE]
# Preface
In the section we are required to optimize a `DNN(deep neural network)`
program based on a standalone hybrid CPU+MIC platform. The detailed
configuration is as follows: 

![2016-02-18 23:01:13屏幕截图]

[2016-02-18 23:01:13屏幕截图]: images/2016-02-18-23-01-13-.png "2016-02-18 23:01:13屏幕截图" { width:auto; max-width:90% }

![2016-02-18 23:13:11屏幕截图]

[2016-02-18 23:13:11屏幕截图]: images/2016-02-18-23-13-11-.png "2016-02-18 23:13:11屏幕截图" { width:auto; max-width:90% }



# Analysis of the serial program
First, we generate a call graph by using `Google perfools`,
 a open source performance profiler, to have a glance though it. Every
 square represents a function, and the bigger square is, the more time
 corresponding function cost.
 

![100001994364201]

[100001994364201]: images/100001994364201.jpg "100001994364201" { width:auto; max-width:90% }

Obviously, the hot spot is something about `MKL`. After googling
and searching Intel document we know that MKL provides `BLAS routinues`,
which includes a serial funcition named `cblas_?gemm`
to compute a matrix-matrix product with general matrices.

Then we search for `cblas_*gemm`, results show the usage of `cblas_*gemm`
 appear in file `dnn_func.cpp`, more specifically, in three function:

* ` extern "C" int dnnForward(NodeArg &nodeArg)`
* ` extern "C" int dnnBackward(NodeArg &nodeArg)`
* ` extern "C" int dnnUpdate(NodeArg &nodeArg)`

So we guess that those function is what we should optimize, aka, hotspots.
The report showed by`Intel VTune`, another profiler, proves our guess.

After a skim through the source code, we have a a clear structure 
about the program. To simplify our describe, original program 
could be rewritten in pseudocode:

``` Java
1. GetInitFileConfig(cpuArg)
2. While FetchOneChunk(cpuArg, onChunk) do:
         While FetchOneBunch(oneChunk, nodeArg) do:
              dnnForward(nodeArg)
              dnnBackward(nodeArg)
              dnnUpate(nodeArg)
3. WriteWts(nodeArg, cpuArg)
4. UninitProgramConfig(cpuArg)

```
