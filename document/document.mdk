Title         : Optimization of a DNN Program on the CPU+MIC Platform
Logo          : False
Bibliography: reference.bib
Author      : Liwei Cui
Affiliation : University of Electronic Secience and Technology of China
Email       : 2014060101003@std.uestc.edu.cn
Title Note  : &date;
Sub Title   : Competition proposal of ASC

[TITLE]

~ Abstract
This article is a part of competition proposal of Asia Supercomputer Student Challenge. We analysis the `DNN` program, discuss different design methods, and show optimization methods with their efforts.
~

[TOC]

# Introduction
There is a program based on a standalone hybrid CPU+MIC platform called `DNN(deep neural network)` needed to be parallelized for obtain better performance. We could use any parallel programming mode supported by `MIC` to achieve this goal. Here is some detailed information about hardware in Figure 1, software configuration in Figure 2.

After optimization, the final program is tested on one computing server on the CPU+MIC hybrid cluster. Performance analysis in this proposal is based on the results of this test. 
~ Figure { #fig-myfigure; caption:"Hardware configuration"}

![2016-02-18 23:01:13屏幕截图]

[2016-02-18 23:01:13屏幕截图]: images/2016-02-18-23-01-13-.png "2016-02-18 23:01:13屏幕截图" { width:auto; max-width:90% }
~

~ Figure { #fig-myfigure; caption:"Software configuration"}
![2016-02-18 23:13:11屏幕截图]

[2016-02-18 23:13:11屏幕截图]: images/2016-02-18-23-13-11-.png "2016-02-18 23:13:11屏幕截图" { width:auto; max-width:90% }
~

# Analysis of the Serial Program
## Coarse Grain Analysis
At first, we generate a call graph(Figure 3) by using `Google perfools`, a open source performance profiler, to have a glance though it. Every square represents a function, and the bigger square is, the more time corresponding function costs.
 
~ Figure { #fig-myfigure; caption:"Google Perfools results"}
![100001994364201]

[100001994364201]: images/100001994364201.jpg "100001994364201" { width:auto; max-width:90% }
~
Obviously, the hot spot is something about `MKL`. After googling
and searching Intel document we know that MKL provides `BLAS routinues`, which includes a serial function named `cblas_?gemm` to compute a product of general matrices.

But giving that MKL function is well-optimized[@sgemm], we search for all position where `cblas_*gemm` is called. Results show the usage of `cblas_*gemm` appear in file `dnn_func.cpp`, more specifically, in three functions:

* ` extern "C" int dnnForward(NodeArg &nodeArg)`
* ` extern "C" int dnnBackward(NodeArg &nodeArg)`
* ` extern "C" int dnnUpdate(NodeArg &nodeArg)`

~ Figure { #fig-myfigure; caption:"Intel VTune top-down tree"}
![2016-02-19 10:36:25屏幕截图]
[2016-02-19 10:36:25屏幕截图]: images/2016-02-19-10-36-25-.png "2016-02-19 10:36:25屏幕截图" { width:auto; max-width:90% }
~

They call `MKL` function `cblas_sgemm` many times in their `for loop` and 
cost almost 90% of all CPU time. So we guess that those function
 is what we may optimize, aka, hotspots. The report(see Figure 4.) showed by`Intel VTune`, another profiler, proves our guess.

According to a skim through the source code, we could establish a clear structure about this program. To simplify code, original program could be rewritten in pseudocode:
``` Java
   GetInitFileConfig(cpuArg)
   While FetchOneChunk(cpuArg, onChunk) do:
           While FetchOneBunch(oneChunk, nodeArg) do:
                dnnForward(nodeArg)
                dnnBackward(nodeArg)
                dnnUpate(nodeArg)
    WriteWts(nodeArg, cpuArg)
    UninitProgramConfig(cpuArg)

```
There are two nested loop surrounding `dnn*()` series, and 
in each of those processing function some matrix-matrix product are
calculated. Whether those hotspots could be parallelized or not depends
on data scale, dependency and so on. Before we discuss some methods and weighed their pros and cons the implementation of `DNN` should be carefully checked.

## Fine Grain Analysis

### Matrix Size
Here is an example of how `DNN` use `cblas_sgemm`:

``` java
  cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans,\
        numN, numA[i], numA[i-1], \
        one, d_Y[i-1], numA[i-1], d_W[i], numA[i],
        one, d_Y[i], numA[i]);
```
In this way we could calculate `d_Y[i-1]` * `d_W[i]` + `d_Y[i]` and assign the result to `d_Y[i]`. The arguments `numN`, `numA[i]`, `numA[i-1]` indicating the size of the matrices:

 * `d_Y[i-1]` is a `numN` row by `numA[i]` column matrix;
 * `d_W[i]` is a `numN` row by `numA[i-1]` column matrix;
 * `d_Y[i]` is a `numA[i-1]` row by `numA[i]` column matrix.

As we known the bigger matrix size is, the higher degree of `MKL` parallelism is[@paraMKL]. But in the `DNN` program, the size of matrix is decided by `bunchSize`, a constant integer (&asymp;1024), and a element (&asymp;1024) of `dnnLayerArr`, which is a constant integer array. The two integers are set by configuration file, and we are not allowed to modify it. For this reason there are no sufficiently large matrix to enable `auto offload model` of `MIC` to speed up `DNN`.[@MKL-MIC]

### Cycles Index
In the `dnn*` series every loop call `cblas_sgemm` `numN`(&asymp;7) times, which indicates the length of `dnnLayerArr`. It's regretful that the value also cannot be modified by us. Giving the number of core(&asymp;24 on CPU or &asymp;60 on MIC) and constant `numN`, we couldn't improve performance a lot by parallelizing those loops.

# Parallelization Design Methods

## Fine Grain Parallelism

In function `dnnForward`, it's easy to observe there is a loop invoking `cblas_sgemm`, which nearly cost all CPU time consumed by this function. So it's same with the `dnnBackward` and `dnnUpdate`. A rough thoughts occurred to us that we could parallelize this `for loop` using multi-threads. But data dependency in `for loop` and relatively small cycle index make it inefficient to parallelize. So   we should consider generalizing the parallelization region to smaller scale. Taking the feature of `MIC` in to account we hope to execute highly parallel code and/or compute intensive code on the `MIC`. Focusing on `dnn*` series it's easy to find an abstract structure to generalize them:
```java
  extern "C" int dnn***(NodeArg &nodeArg)
  {
     /* Variables definition */
     float  *d_X = nodeArg.d_X;
	   ...
      /* Preprcess function func1 */
      func1(...);

    /* a for loop where a function, a MKL \
    and another function are invoked oderly */
    for (int i = num; ...) {
          func2(...);
          cblas_sgemm(CblasRowMajor, CblasNoTrans, ...);
          func3(...);
          }
      } 
      return 0;
  }

``` 
Arguments of `func1`, `func2`, `func3` is matrix or vector, which is easy to parallelized. Taking all factors into consider, we have two choose: a) use serial `MKL` then parallelize the whole `for loop`, b) use multi-thread `MKL` and parallelize `func1, 2, 3`. The two measures all support `MIC`, but we prefer the b) because we could benefit from the parallel optimization of `MKL` and cooperation among `MKL` and `MIC`.

~ Figure { #fig-figure; caption:"`DNN` structure"}
![2016-02-21 21:15:24屏幕截图]

[2016-02-21 21:15:24屏幕截图]: images/2016-02-21-21-15-24-.png "2016-02-21 21:15:24屏幕截图" { width:auto; max-width:90% }
~


## Coarse Grain Parallelism
To implenment coarse grain parallelism we hope that each thread/process finish large subcomponents. To achieve this goal `DNN` program should be divided into (mostly) independent and similar proportions, and every proportion should be as large as possible. But considering the structure of `DNN` the default ordering of `dnn*` series couldn't be changed, neither does the processing of file-reading. So in our opinion it's difficult to implement coarse grain parallelism without any change to `DNN` structure and its dataset.

# Performance Optimization Methods

## Compiler Assisted Offload

### Scope of Offloaded Sections
We have talked about the common structure of `DNN` series. In their `for` loop there are serial processing and `MKL` function appear alternately, and serial processing modifies data which is used as input in next `cblas_sgemm`function. Data change between `MIC` and `CPU` is slow because of `PCI-E` speeds so the best known methods[@effective_use] is to make the whole section of code an offload unit, which could reduce most of data transfer. In addition, because we nearly make the three `dnn` functions offload units, data transfer between them is less. SO, all we need do is to transfer input data in the start of `for loop` from `CPU` to `MIC` and transfer output after loop to `CPU`. `dnn` Function is like this (issues about array transfer discussed below):
```java
  extern "C" int dnn***(NodeArg &nodeArg)
  {
    /* Variables definition */
    float  *d_X = nodeArg.d_X;
	  ...
    
    #pragma offload target(mic)\
    in(pd_Y:length(0) REUSE)\
    in(pd_W:length(0) REUSE)\
    ...
    {
      /* Preprcess function func1 */
      func1(...);

      /* a for loop where a function, a MKL \
      and another function are invoked oderly */

      for (int i = num; ...) {
          func2(...);
          cblas_sgemm(CblasRowMajor, CblasNoTrans, ...);
          func3(...);
      }
    } 
    return 0;
  }

```

### Dimensionality Reduction
As we know, `MIC` disallow specifying array of pointer, aka. multi-dimension array, to be use in `in` or `out` clauses. To cap it all element in those arrays used in `DNN` aren't stored consecutively in memory and have different size. So it's necessary for us to calculate array size to allocate one-dimension array. Code like this:
```java
    W_len = 0;
    /* calculate element size */
    for (int i = 1; i < dnnLayerNum; i++) {
        W_pos[i - 1] = W_len;
        W_len += /* i-st element size */;
    }
    /* allocate memory */
    pd_W = (float *)mkl_malloc(W_len * sizeof(float), 64);
    /* build two-dimension array */
    for (int i = 0; i < dnnLayerNum - 1; i++) {
        d_W[i] = pd_W + W_pos[i];
    }

```

## `OpenMP` Optimization
Though `MKL` function could decide whether to be threaded after a runtime check[@paraMKL; @MKL_threads], other serial functions still run slowly in it. To maximize the utilization it's better to have more multithreading processing[@effective_use]. In pseudocode we discuss in `4.1.1`, `fun1`, `fun2` and `fun3` are bottleneck. Those function all take one or more matrices as arguments, access and modify them then return nothing. Without data dependency it is easy to parallelize them using `OpenMp`:
```java
  extern "C" int errorOutput(float *E, float *Z, ...)
  {
	 float tmp;
	 int idx, j;
    //OpenMp block 
    #pragma omp parallel for private(tmp, idx, j)
	 for(int i=0; i<row; i++)
	 {
		  for(j=0; j<col; j++)
		  {
			   idx = i*col + j;
			   tmp = (j == T[i]) ? 1.0f:0.0f;
			   E[idx] = Z[idx] - tmp;
		  }
	 }
	 return 0;
  }

```

## Alignment
Alignment is important for `MKL` function to optimize program further and for compiler to assist vectorization. For example, allocating memory for matrices aligned on 64-byte boundary(or more) allows `cblas_sgemm` to have better performance. Using `mkl_alloc` is easy to allocate or deallocate an aligned `2MB(0x200000B)` memory buffer :
 ```java
    float *d_X = (float *)mkl_malloc(N * sizeof(float), 0x200000);
    ...
    mkl_free(d_x);
 ```
Data transfer rate between `CPU` and `MIC` is slow, so align CPU data on a 64B boundary or higher is necessary for improve data transfer rate[@effective_use]. Align at 2MB for maximum transfer rate, so we use `align modifier` to improve it and enable vectorization of code on `MIC`:
```java
    #pragma offload target(mic) in(pd_X: length(X_len) align(0x200000))
    ...
```

## Environmental Variables Settings
### `MKL_DYNAMIC=true`:  
This variable leads to a dynamic reduction of number of `OpenMPI` threads based on analysis of system workload, so it may reduce possible oversubscription from MKL threading.

### `MKL_NUM_THREADS=57 OMP_NUM_THREADS=57`:  
Those variables decide a maximum threads allowed to create. Since parallelization regions of this program run in the `MIC` which has 57 cores in test platform, we set those variables equal to 57. 

### `MIC_USE_2MB_BUFFERS=64K`  
2MB pages are needed to improve transfer rate between `MIC` and `CPU`.[@page_file]

# Testing Process and Results on the CPU+MIC Platform
Our final program is tested with the `workload2` on `CPU+MIC` hybrid cluster for many times(&ge;5) and analysis in this proposal is based on those results. After averaging the time our `DNN` program costs and verifying the correctness of results, a table is created and showed below. The source code folder `dnntk_src` and log folder `exp` are packed in the file `DNN.zip`.

| --------------------------| ----------------| --------------------|--------------|
| Methods                                    ||                     |              |
| \/------------------------| ----------------|                     |              |
| Description               |`OpenMP` Position|Time (ms)            |&emsp;Speedup |
| --------------------------| :---:           | -------------------:|---:          |
| `CPU`                     | \\              | &emsp;1,533,962.250 | 1.00         |
| `MIC `                    | \\              | 7,368,252.500       | 0.28         |
| `OpenMP `                 | `CPU`           | 366,702.218         | 4.18         |
| `MIC` with `OpenMP` &emsp;| `CPU`           | 33.33               |              |
| Final Result              | `MIC`           |333,205.375          | 4.60         |
| --------------------------| ----------------| --------------------|--------------|
{ .booktable }


[BIB]